# Stakeholder Interview #6: Knowledge Manager Persona

**Participant:** Samantha Lee
**Role:** Knowledge Manager, Technology Practice
**Location:** San Francisco, CA
**Tenure:** 9 years at Accenture
**Responsibility:** Content curation and knowledge strategy for Technology practice
**Products Used:** AKX (daily, admin role)
**Date:** February 12, 2025
**Interviewer:** Senior Product Owner
**Duration:** 40 minutes

---

## Background

Samantha is one of the knowledge managers responsible for maintaining content quality and curation within the Technology practice. She's the person who curates "gold standard" content, manages content lifecycle, and helps partners find what they need. She has a unique perspective on both the content and user sides of AKX.

---

## Interview

**Q: Tell me about your role with AKX.**

Samantha: "I'm the person who tries to make AKX work despite its limitations. My job is to curate content - identify the best deliverables, tag them properly, maintain quality collections. I also help partners and consultants find things when search fails them. Think of me as the human search engine behind AKX."

**Q: What's your daily workflow like?**

Samantha: "I spend about 40% of my time answering 'help me find' requests. Someone needs a cloud migration assessment template, they can't find it, they email me. I dig through AKX, my personal archives, my network. The other 40% is curation - reviewing new uploads, tagging, retiring old content. The remaining 20% is reporting and strategy that nobody reads."

**Q: Why does search fail people so often?**

Samantha: "Multiple reasons. First, people upload content with terrible metadata. The document is titled 'Final_v3_client_edited.pptx' with no tags. Second, the search algorithm is basic keyword matching - it doesn't understand context or intent. Third, we have 2.3 million documents in the system, of which maybe 50,000 are actually useful. The signal-to-noise ratio is terrible."

**Q: What would AI search change for you?**

Samantha: "Everything. If AI search actually works - if it can understand 'I need a digital transformation roadmap for a regional bank' and return the 5 best examples - my job transforms. Instead of being a human search engine, I can focus on curation strategy, quality improvement, knowledge capture. Right now I'm the band-aid for a broken system."

**Q: Tell me about content quality issues.**

Samantha: "We have content from 2010 still showing up in search results. We have duplicate versions of the same framework uploaded 20 times. We have client-confidential content that was uploaded without redaction. We have brilliant thought leadership that no one can find because it was tagged wrong. It's chaos. I clean up what I can but the scale is overwhelming."

**Q: How do you identify 'gold standard' content?**

Samantha: "Manually. I review deliverables, I ask subject matter experts to recommend their best work, I look at what partners request repeatedly. Then I tag it, add it to curated collections, feature it in newsletters. But this is maybe 2% of what gets uploaded. 98% of content goes into the void, never properly curated, never found again."

**Q: What about expert profiles and skills data?**

Samantha: "Terrible quality. People don't update their skills. When they do, they over-claim - everyone says they have 'AI expertise' because they attended one webinar. There's no validation. I've pushed for skills to be auto-populated from project assignments, but that integration doesn't exist. So we have unreliable skill data feeding an unreliable expert search."

**Q: What feedback do you hear from users?**

Samantha: "Two things, constantly. First: 'I can't find anything.' Second: 'The stuff I find is garbage.' They're not wrong. Search is broken. Quality control is impossible at scale. The consultants who succeed are the ones who've been here long enough to build personal networks and know who to call. That's not scalable, and it disadvantages new hires."

**Q: What would you change about AKX?**

Samantha: "AI-powered search that actually works. Automatic content quality scoring - surface the good stuff, hide the garbage. Skills validation based on real project work. And honestly, delete half the content. Most of it is noise. We'd be better off with 100,000 high-quality documents than 2.3 million random uploads."

---

## Key Insights
- Knowledge manager serves as "human search engine" - 40% of time answering find requests
- 2.3 million documents, only ~50,000 are useful (2% signal, 98% noise)
- Search fails due to: poor metadata, keyword-only matching, overwhelming volume
- Content quality issues: outdated content, duplicates, confidential data, poor tagging
- Skills data is unreliable - people over-claim, no validation
- Manual curation identifies "gold standard" but can only cover 2% of uploads
- AI search would transform role from reactive to strategic
- Users consistently complain: "Can't find anything" and "What I find is garbage"
- Suggests aggressive content cleanup - quality over quantity
- New hires disadvantaged - success depends on personal networks built over time
